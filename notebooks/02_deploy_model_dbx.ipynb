{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcef310",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.50.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e9a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72412e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"../configs/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "catalog_name = config.get(\"catalog_name\")\n",
    "schema_name = config.get(\"schema_name\")\n",
    "volume_name = config.get(\"volume_name\")\n",
    "volume_folder = config.get(\"volume_folder\")\n",
    "model_name = config.get(\"model_name\")\n",
    "revision = config.get(\"revision\")\n",
    "uc_model_name = f\"{catalog_name}.{schema_name}.{model_name.split(\"/\")[-1]}\"\n",
    "served_model_name = config.get(\"served_model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94343856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_volume =  f\"/Volumes/{catalog_name}/{schema_name}/{}/{volume_name}/{revision}/{volume_folder}\"\n",
    "cache_hf = \"/local_disk0/hf_cache\"\n",
    "cache_local = \"/local_disk0/{volume_folder}\" \n",
    "\n",
    "os.environ[\"HF_HOME\"] = cache_hf\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_hf\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"True\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"1000\"\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'  # Enables optimized download backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Copy volume cache to local cache if not already there\n",
    "if not os.path.exists(cache_local):\n",
    "    try: \n",
    "        print(f\"Loading model from {cache_volume} to {cache_local}.\")\n",
    "        snapshots_dir = '/'.join(cache_local.split('/')[:-1])\n",
    "        if not os.path.exists(snapshots_dir):\n",
    "            os.makedirs(snapshots_dir)\n",
    "        \n",
    "        shutil.copytree(cache_volume, cache_local) \n",
    "        print(f\"Successfully loaded model from {cache_volume} to {cache_local}!\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"File exists locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b62f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cache_local)\n",
    "\n",
    "# Set pad_token \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cache_local,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# TODO: Update class name to your preferred name\n",
    "class HFModelPyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "\n",
    "        self.model_id = context.artifacts[\"model-weights\"] \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dtype  = torch.bfloat16 if self.device == \"cuda\" else torch.float32   \n",
    "\n",
    "        print(\"************************************\")\n",
    "        print(f\"Device: {self.device}, dtype: {self.dtype}\")\n",
    "        print(f\"Loading model {self.model_id} to {self.device}\")\n",
    "        print(\"************************************\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=self.dtype\n",
    "        ).to(device=self.device)\n",
    "\n",
    "    def predict(self, model_input: pd.DataFrame, params: dict = None) -> pd.Series:\n",
    "        outputs = []\n",
    "        max_tokens = params.get(\"max_tokens\", 1024) if params else 1024\n",
    "        \n",
    "        for _, row in model_input.iterrows():\n",
    "            # TODO: Update system_prompt and user_prompt \n",
    "            system_prompt = row.get(\"system_prompt\", \"You are a helpful medical assistant.\")\n",
    "            user_prompt = row.get(\"user_prompt\", \"\")\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "\n",
    "            raw_inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            inputs = {k: (v.to(self.dtype) if v.is_floating_point() else v) for k, v in raw_inputs.items()}\n",
    "\n",
    "            prompt_len = inputs[\"input_ids\"].size(-1)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                generation = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "\n",
    "            generated_tokens = generation[0][prompt_len:]\n",
    "            text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            outputs.append(text)\n",
    "\n",
    "        return pd.Series(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0292a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"system_prompt\": [\"You are a helpful medical assistant.\"],\n",
    "    \"user_prompt\": [\"What are the symptoms of hypertension?\"]\n",
    "})\n",
    "\n",
    "class Context:\n",
    "    def __init__(self, artifacts):\n",
    "        self.artifacts = artifacts\n",
    "\n",
    "custom_context = Context(artifacts={\"model-weights\": str(cache_local)})\n",
    "\n",
    "custom_hf_model = HFModelPyfunc()\n",
    "custom_hf_model.load_context(custom_context)\n",
    "\n",
    "output = custom_hf_model.predict(df)\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c33a24",
   "metadata": {},
   "source": [
    "%md\n",
    "### Bug when logging artifacts where model serving will not use the full path:\n",
    "- Fix: When using a PythonModel during serving, you access artifact files with paths like context.artifacts['artifact-key']; these keys correspond to directories or files under `<model_root>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c82778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21080e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will infere signature from the input and output dataframes\n",
    "signature = infer_signature(\n",
    "  model_input=df, \n",
    "  model_output=output,\n",
    "  params={\"max_tokens\": 512}\n",
    "  ) # Doing strict schema to avoid rerunning pipeline\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"hf_model_pyfunc\",\n",
    "        python_model=HFModelPyfunc(),\n",
    "        signature=signature,\n",
    "        pip_requirements=\"requirements.txt\",\n",
    "        # extra_pip_requirements=package_versions,  \n",
    "        artifacts={\n",
    "            'model-weights': cache_local},\n",
    "        input_example = df\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31292441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=model_info.model_uri,\n",
    "        name=uc_model_name\n",
    "    )\n",
    "    print(f\"Registered model version: {result.version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error registering model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "client.set_registered_model_alias(\n",
    "    name=uc_model_name,\n",
    "    alias=\"challenger\",\n",
    "    version=result.version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "model_uri = f\"models:/{uc_model_name}@challenger\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51444f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4441e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"system_prompt\": [\"You are a helpful medical assistant.\"],\n",
    "    \"user_prompt\": [\"What are the symptoms of hypertension?\"]\n",
    "})\n",
    "\n",
    "outputs = loaded_model.predict(df, params={\"max_tokens\": 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c164f",
   "metadata": {},
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "        EndpointCoreConfigInput,\n",
    "        ServedEntityInput,\n",
    "        AutoCaptureConfigInput,\n",
    "        ServingEndpointDetailed,\n",
    "        ServingModelWorkloadType,\n",
    "        EndpointTag\n",
    "    )\n",
    "\n",
    "model_version = client.get_model_version_by_alias(uc_model_name, \"Champion\")\n",
    "served_entity_name = {model_name.split(\"/\")[-1]}\n",
    "user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "served_entities = [\n",
    "    ServedEntityInput(\n",
    "        entity_name=model_name,\n",
    "        entity_version=model_version,\n",
    "        name=served_entity_name,\n",
    "        workload_type=ServingModelWorkloadType.GPU_LARGE,\n",
    "        workload_size=\"Small\",\n",
    "        scale_to_zero_enabled=True,\n",
    "    )\n",
    "]\n",
    "auto_capture_config = AutoCaptureConfigInput(\n",
    "    catalog_name=catalog_name,\n",
    "    schema_name=schema_name,\n",
    "    table_name_prefix=f\"{model_name}_serving\",\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "endpoint_details = w.serving_endpoints.create_and_wait(\n",
    "            name=f\"{served_entity_name}_endpoint\",\n",
    "            config=EndpointCoreConfigInput(\n",
    "                name=f\"{served_entity_name}_endpoint\",\n",
    "                served_entities=served_entities,\n",
    "                auto_capture_config=None\n",
    "            ),\n",
    "            tags=[\n",
    "                EndpointTag(key=\"application\", value=served_entity_name),\n",
    "                EndpointTag(key=\"created_by\", value=user_email)\n",
    "            ],\n",
    "            timeout = timedelta(minutes=180) # wait up to three hours\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

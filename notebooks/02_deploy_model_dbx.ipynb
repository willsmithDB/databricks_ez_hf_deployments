{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21285c5b-0e29-41d2-a1c6-db0240f989a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a61d846-1b1e-43a3-aa95-350275572ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.50.0\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f81f7fb-4800-4acd-8fe1-607f522bc021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import requests, torch\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cbcfe6-c587-40ad-897c-bacfed74394c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"../configs/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "catalog_name = config.get(\"catalog_name\")\n",
    "schema_name = config.get(\"schema_name\")\n",
    "volume_name = config.get(\"volume_name\")\n",
    "volume_folder = config.get(\"volume_folder\")\n",
    "model_name = config.get(\"model_name\")\n",
    "revision = config.get(\"revision\")\n",
    "uc_model_name = f\"{catalog_name}.{schema_name}.{model_name.split(\"/\")[-1]}\"\n",
    "served_model_name = config.get(\"served_model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15ea5c8-c862-4dda-9aec-3cc153da45e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cache_volume =  f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/{model_name}/{revision}/{volume_folder}\"\n",
    "cache_hf = \"/local_disk0/hf_cache\"\n",
    "cache_local = f\"/local_disk0/{volume_folder}\" \n",
    "\n",
    "os.environ[\"HF_HOME\"] = cache_hf\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_hf\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"True\"\n",
    "os.environ[\"HF_HUB_DOWNLOAD_TIMEOUT\"] = \"1000\"\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'  # Enables optimized download backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2548a3-7b5b-4106-9beb-18b1d9055f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Copy volume cache to local cache if not already there\n",
    "if not os.path.exists(cache_local):\n",
    "    try: \n",
    "        print(f\"Loading model from {cache_volume} to {cache_local}.\")\n",
    "        snapshots_dir = '/'.join(cache_local.split('/')[:-1])\n",
    "        if not os.path.exists(snapshots_dir):\n",
    "            os.makedirs(snapshots_dir)\n",
    "        \n",
    "        shutil.copytree(cache_volume, cache_local) \n",
    "        print(f\"Successfully loaded model from {cache_volume} to {cache_local}!\")\n",
    "    except Exception as e: \n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"File already exists locally at {cache_local}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11a18a9-14b9-4c25-b348-15f24de72670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(cache_local)\n",
    "\n",
    "# # Set pad_token \n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     cache_local,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8211addb-53b6-4e28-836a-84b75b88bc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import mlflow.pyfunc\n",
    "\n",
    "# TODO: Update class name to your preferred name\n",
    "class HFModelPyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "\n",
    "        self.model_id = context.artifacts[\"model-weights\"] \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dtype  = torch.bfloat16 if self.device == \"cuda\" else torch.float32   \n",
    "\n",
    "        print(\"************************************\")\n",
    "        print(f\"Device: {self.device}, dtype: {self.dtype}\")\n",
    "        print(f\"Loading model {self.model_id} to {self.device}\")\n",
    "        print(\"************************************\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=self.dtype\n",
    "        ).to(device=self.device)\n",
    "\n",
    "        print(\"************************************\")\n",
    "        print(f\"Successfully loaded model and tokenizer to {self.device}\")\n",
    "        print(\"************************************\")\n",
    "\n",
    "\n",
    "    def predict(self, model_input: pd.DataFrame, params: dict = None) -> pd.Series:\n",
    "        outputs = []\n",
    "        max_tokens = params.get(\"max_tokens\", 1024) if params else 1024\n",
    "        \n",
    "        for _, row in model_input.iterrows():\n",
    "            # TODO: Update system_prompt and user_prompt \n",
    "            system_prompt = row.get(\"system_prompt\", \"You are a helpful medical assistant.\")\n",
    "            user_prompt = row.get(\"user_prompt\", \"\")\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "\n",
    "            raw_inputs = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            inputs = {k: (v.to(self.dtype) if v.is_floating_point() else v) for k, v in raw_inputs.items()}\n",
    "\n",
    "            prompt_len = inputs[\"input_ids\"].size(-1)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                generation = self.model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False)\n",
    "\n",
    "            generated_tokens = generation[0][prompt_len:]\n",
    "            text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            outputs.append(text)\n",
    "\n",
    "        return pd.Series(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23509f49-d9b4-4239-a194-a31641b8a119",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"system_prompt\": [\"You are a helpful medical assistant.\"],\n",
    "    \"user_prompt\": [\"What are the symptoms of hypertension?\"]\n",
    "})\n",
    "\n",
    "class Context:\n",
    "    def __init__(self, artifacts):\n",
    "        self.artifacts = artifacts\n",
    "\n",
    "custom_context = Context(artifacts={\"model-weights\": str(cache_local)})\n",
    "\n",
    "custom_hf_model = HFModelPyfunc()\n",
    "custom_hf_model.load_context(custom_context)\n",
    "\n",
    "output = custom_hf_model.predict(df)\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "545a552b-18da-4b89-982b-79b173e48af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Bug when logging artifacts where model serving will not use the full path:\n",
    "- Fix: When using a PythonModel during serving, you access artifact files with paths like context.artifacts['artifact-key']; these keys correspond to directories or files under `<model_root>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa171adb-b5b6-4a75-9570-f1c92877e316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbba91a-7e12-4daf-8917-097afbb68a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This will infere signature from the input and output dataframes\n",
    "signature = infer_signature(\n",
    "  model_input=df, \n",
    "  model_output=output,\n",
    "  params={\"max_tokens\": 512}\n",
    "  ) # Doing strict schema to avoid rerunning pipeline\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"hf_model_pyfunc\",\n",
    "        python_model=HFModelPyfunc(),\n",
    "        signature=signature,\n",
    "        pip_requirements=\"requirements.txt\",\n",
    "        # extra_pip_requirements=package_versions,  \n",
    "        artifacts={\n",
    "            'model-weights': cache_local},\n",
    "        input_example = df\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4393c534-4228-400b-9e2e-d80c69f84efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    result = mlflow.register_model(\n",
    "        model_uri=model_info.model_uri,\n",
    "        name=uc_model_name\n",
    "    )\n",
    "    print(f\"Registered model version: {result.version}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error registering model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4798aa9-e362-49f7-b0a6-7f099c88707c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "client.set_registered_model_alias(\n",
    "    name=uc_model_name,\n",
    "    alias=\"challenger\",\n",
    "    version=result.version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92717db6-a0cb-4e6c-8208-fe2b36f3fa56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "model_uri = f\"models:/{uc_model_name}@challenger\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce37e5f-bc7d-4f21-adaf-ea179da6e71b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Can restart the session (%restart_python) and reload config variables if needed for VRAM requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e43ff65-a64f-4e65-bf72-ed89f016a383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac08506-3cc2-4c59-8fbf-df8f238901bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"system_prompt\": [\"You are a helpful medical assistant.\"],\n",
    "    \"user_prompt\": [\"What are the symptoms of hypertension?\"]\n",
    "})\n",
    "\n",
    "outputs = loaded_model.predict(df, params={\"max_tokens\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c75f3d-921f-4fc3-8bbb-7e7f366063fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if outputs[0] is not None:\n",
    "  print(outputs[0])\n",
    "  client.set_registered_model_alias(\n",
    "      name=uc_model_name,\n",
    "      alias=\"Champion\",\n",
    "      version=result.version\n",
    ")\n",
    "  print(\"***************************\")\n",
    "  print(\"Model promoted to Champion!\")\n",
    "  print(\"***************************\")\n",
    "\n",
    "else:\n",
    "  print(\"Model failed to load so cannot promote to Champion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1b31d8-3aa3-44ea-ba3e-f34e94ce9f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(client.get_model_version_by_alias(uc_model_name, \"Champion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c0f89b-b8ae-4298-ac75-896532dffe04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_info = client.get_model_version_by_alias(uc_model_name, \"Champion\")\n",
    "model_name = model_info.name\n",
    "model_version = model_info.version\n",
    "served_entity_name = served_model_name\n",
    "user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce773544-e835-4525-84c3-89eb52ffa4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import (\n",
    "        EndpointCoreConfigInput,\n",
    "        ServedEntityInput,\n",
    "        AutoCaptureConfigInput,\n",
    "        ServingEndpointDetailed,\n",
    "        ServingModelWorkloadType,\n",
    "        EndpointTag\n",
    "    )\n",
    "\n",
    "served_entities = [\n",
    "    ServedEntityInput(\n",
    "        entity_name=model_name,\n",
    "        entity_version=model_version,\n",
    "        name=served_entity_name,\n",
    "        workload_type=ServingModelWorkloadType.GPU_LARGE,\n",
    "        workload_size=\"Small\",\n",
    "        scale_to_zero_enabled=True,\n",
    "    )\n",
    "]\n",
    "auto_capture_config = AutoCaptureConfigInput(\n",
    "    catalog_name=catalog_name,\n",
    "    schema_name=schema_name,\n",
    "    table_name_prefix=f\"{model_name}_serving\",\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "endpoint_details = w.serving_endpoints.create_and_wait(\n",
    "            name=f\"{served_entity_name}_endpoint\",\n",
    "            config=EndpointCoreConfigInput(\n",
    "                name=f\"{served_entity_name}_endpoint\",\n",
    "                served_entities=served_entities,\n",
    "                auto_capture_config=None\n",
    "            ),\n",
    "            tags=[\n",
    "                EndpointTag(key=\"application\", value=served_entity_name),\n",
    "                EndpointTag(key=\"created_by\", value=user_email)\n",
    "            ],\n",
    "            timeout = timedelta(minutes=180) # wait up to three hours\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5377ac8e-c13c-4e0c-8dd7-4a00270d7233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Restart to clear VRAM from the GPU (only for this notebook session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1205e7-aaf0-48a8-ab32-6692e2408287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fca97a6-b47c-414e-9ce1-5abf5c4b0aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_deploy_model_dbx",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
